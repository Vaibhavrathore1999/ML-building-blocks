{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQiEWNpdwrWZIZd+7OkW+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhavrathore1999/ML-building-blocks/blob/main/ViT_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j6-_EAXs7iwv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GELU,self).__init__()\n",
        "    pass\n",
        "  def forward(self,x):\n",
        "    return torch.tensor(0.5*x*(1+torch.tanh(torch.tensor(2/math.pi)**0.5)*(x+0.044715*torch.pow(x,3.0))))\n"
      ],
      "metadata": {
        "id": "QK-XWdBXolIR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Image into Patches"
      ],
      "metadata": {
        "id": "iBUTghrbuKCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self,img_size,patch_size,in_channels,emb_size):\n",
        "    assert (img_size%patch_size==0), \"image size must be divisible by patch size\"\n",
        "    super(PatchEmbeddings,self).__init__()\n",
        "    self.img_size=img_size\n",
        "    self.patch_size=patch_size\n",
        "    self.n_patches=(img_size//patch_size)**2\n",
        "    # Convert each patch into embedding\n",
        "    self.proj=nn.Conv2d(in_channels,emb_size,kernel_size=(patch_size,patch_size),stride=patch_size)\n",
        "  def forward(self,x):\n",
        "    # Shape of x ---> (batch_size,num_channels,image_size,image_size). get converted to (batch_size,num_patches,emb_dim)\n",
        "    x=self.proj(x)      # ----> (batch_size,emb_dim,img_size//patch_size,img_size//patch_size)\n",
        "    x=x.flatten(2)      # ----> (batch_size,emb_dim,num_patches)\n",
        "    x=x.transpose(1,2)  # ----> (batch_size,num_patches,emb_dim)\n",
        "    return x"
      ],
      "metadata": {
        "id": "qqpotwqxsrf_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings_v2(nn.Module):\n",
        "  def __init__(self,img_size,patch_size,in_channels,emb_size):\n",
        "    assert (img_size%patch_size==0), \"image size must be divisible by patch size\"\n",
        "    super(PatchEmbeddings,self).__init__()\n",
        "    self.img_size=img_size\n",
        "    self.patch_size=patch_size\n",
        "    self.n_patches=(img_size//patch_size)**2\n",
        "    self.linear=nn.Linear(in_channels*patch_size*patch_size,emb_size)\n",
        "  def forward(self,x):\n",
        "    # Shape of x ---> (batch_size,num_channels,image_size,image_size). get converted to (batch_size,num_patches,emb_dim)\n",
        "    patches=x.unfold(2,self.patch_size.self.patch_size).unfold(3,self.patch_size,self.patch_size)\n",
        "    patches=patches.contiguous().view(x.shape[0,self.n_patches,-1])\n",
        "    return self.linear(patches)\n"
      ],
      "metadata": {
        "id": "cPG3mQ-UuNyU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Learnable Positional Embeddings"
      ],
      "metadata": {
        "id": "6PAJhgCG4Jvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self,img_size,patch_size,in_channels,emb_size,dropout):\n",
        "    super(Embedding,self).__init__()\n",
        "    self.patch_embeddings=PatchEmbeddings(img_size,patch_size,in_channels,emb_size)\n",
        "    # Create a learnable [CLS] Token\n",
        "    self.cls_token=nn.Parameter(torch.randn(1,1,emb_size))\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.pos_embbedings=nn.Parameter(torch.randn(1,self.patch_embeddings.n_patches+1,emb_size))\n",
        "  def forward(self,x):\n",
        "    x=self.patch_embeddings(x)\n",
        "    cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "    x=torch.cat((cls_tokens,x),dim=1)\n",
        "    x=x+self.pos_embeddings\n",
        "    x=self.dropout(x)"
      ],
      "metadata": {
        "id": "SnaLna6wz1xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Fixed sine and cosine pos embeddings"
      ],
      "metadata": {
        "id": "js0QjoM34O6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding_v2(nn.Module):\n",
        "  def __init__(self,img_size,patch_size,in_channels,emb_size,dropout):\n",
        "    super(Embedding,self).__init__()\n",
        "    self.patch_embeddings=PatchEmbeddings(img_size,patch_size,in_channels,emb_size)\n",
        "    # Create a learnable [CLS] Token\n",
        "    self.cls_token=nn.Parameter(torch.randn(1,1,emb_size))\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.pos_embbedings=self.get_sinusoidal_positional_embeddings(self.patch_embeddings.n_patches+1,emb_size)\n",
        "\n",
        "  def get_sinusoidal_positional_embeddings(self, num_positions, emb_size):\n",
        "    \"\"\"\n",
        "    Generate sinusoidal positional embeddings using both sine and cosine functions.\n",
        "    \"\"\"\n",
        "    position = torch.arange(0, num_positions, dtype=torch.float).unsqueeze(1)  # (num_positions, 1)\n",
        "    div_term = torch.exp(torch.arange(0, emb_size, 2).float() * -(math.log(10000.0) / emb_size))  # (emb_size / 2)\n",
        "\n",
        "    # Apply sine to even indices (2i)\n",
        "    pos_emb = torch.zeros(num_positions, emb_size)\n",
        "    pos_emb[:, 0::2] = torch.sin(position * div_term)\n",
        "    pos_emb[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pos_emb.unsqueeze(0)  # Shape: (1, num_positions, emb_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.patch_embeddings(x)\n",
        "    cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "    x=torch.cat((cls_tokens,x),dim=1)           # ----> (batch_size,num_patches+1,emb_dim)\n",
        "    x=x+self.pos_embeddings\n",
        "    x=self.dropout(x)"
      ],
      "metadata": {
        "id": "0O4QB4kG4Hwu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self,d_in,d_out,dropout):\n",
        "    self.W_k=nn.Linear(d_in,d_out)        # Shape ---> (d_in,d_out)\n",
        "    self.W_q=nn.Linear(d_in,d_out)\n",
        "    self.W_v=nn.Linear(d_in,d_out)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "  def forward(self,x):\n",
        "    # Shape --> (batch, num_tokens, d_in)\n",
        "    key=x@self.W_k          # Shape --> (batch, num_tokens, d_out)\n",
        "    query=x@self.W_q        # Shape --> (batch, num_tokens, d_out)\n",
        "    value=x@self.W_v        # Shape --> (batch, num_tokens, d_out)\n",
        "    # Shape of attention matrix ----> (batch, num_tokens , num_tokens)\n",
        "    attn_matrix= query@key.transpose(-2,-1)\n",
        "    attn_matrix=attn_matrix/torch.sqrt(torch.tensor(key.shape[-1]))\n",
        "    attn_matrix=torch.softmax(attn_matrix,dim=-1)\n",
        "    attn_matrix=self.dropout(attn_matrix)\n",
        "    context_vec=attn_matrix@value\n",
        "    return context_vec            # # Shape --> (batch, num_tokens, d_out)\n"
      ],
      "metadata": {
        "id": "uDCWLap1-0xB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadWraper(nn.Module):\n",
        "  def __init__(self,d_in,d_out,num_heads,dropout):\n",
        "    super(MultiHeadWraper,self).__init__()\n",
        "    self.num_heads=num_heads\n",
        "    self.d_out=d_out\n",
        "    self.d_in=d_in\n",
        "    self.heads=nn.ModuleList([AttentionHead(d_in,d_out,dropout) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Shape --> (batch, num_tokens, d_in)\n",
        "    x_=torch.cat([self.heads[i](x) for i in range(self.num_heads)],dim=-1)        # Shape ---> (batch,num_tokens,d_out*num_heads)\n",
        "    return x_"
      ],
      "metadata": {
        "id": "wdggu4ODE9Fb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,num_heads,dropout):\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "    self.num_heads=num_heads\n",
        "    self.d_out=d_out\n",
        "    self.d_in=d_in\n",
        "    assert (d_out%num_heads==0), \"d_out should be divisible by num_haeds\"\n",
        "    self.W_k=nn.Linear(d_in,d_out)        # Shape ---> (d_in,d_out)\n",
        "    # self.W_k=nn.Parameter(torch.randn(d_in,d_out))\n",
        "    self.W_q=nn.Linear(d_in,d_out)\n",
        "    self.W_v=nn.Linear(d_in,d_out)\n",
        "    self.dim_head=d_out//num_heads\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Shape ---> (batch, num_tokens, d_in)\n",
        "    keys=self.W_k(x)        # Shape ----> (batch,num_tokens,d_out)\n",
        "    # keys=x@self.W_k\n",
        "    queries=self.W_q(x)     # Shape ----> (batch,num_tokens,d_out)\n",
        "    values=self.W_v(x)      # Shape ----> (batch,num_tokens,d_out)\n",
        "    keys=keys.view(keys.shape[0],keys.shape[1],self.num_heads,self.dim_head)                   # Shape ----> (batch,num_tokens,num_heads,dim_head)\n",
        "    queries=queries.view(queries.shape[0],queries.shape[1],self.num_heads,self.dim_head)       # Shape ----> (batch,num_tokens,num_heads,dim_head)\n",
        "    values=values.view(values.shape[0],values.shape[1],self.num_heads,self.dim_head)           # Shape ----> (batch,num_tokens,num_heads,dim_head)\n",
        "    keys=keys.transpose(1,2)                    # Shape ----> (batch,num_heads,num_tokens,dim_head)\n",
        "    queries=queries.transpose(1,2)               # Shape ----> (batch,num_heads,num_tokens,dim_head)\n",
        "    values=values.transpose(1,2)               # Shape ----> (batch,num_heads,num_tokens,dim_head)\n",
        "    attn_matrix=queries@keys.transpose(-2,-1)     # Shape ---> (batch,num_heads,num_tokens,num_tokens)\n",
        "    attn_matrix=attn_matrix/torch.sqrt(torch.tensor(keys.shape[-1]))\n",
        "    attn_matrix=torch.softmax(attn_matrix,dim=-1)\n",
        "    attn_matrix=self.dropout(attn_matrix)\n",
        "    context_vec=attn_matrix@values        # Shape ---> (batch, num_heads, num_tokens,dim_head)\n",
        "    context_vec=context_vec.transpose(1,2)      # Shape ---> (batch, num_tokens, num_heads,dim_head)\n",
        "    context_vec=context_vec.contiguous().view(context_vec.shape[0],context_vec.shape[1],-1)         # Shape ---> (batch, num_tokens,d_out)\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "YqVPySOHGe5w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_in,hidden_size,dropout):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(d_in,hidden_size)\n",
        "    self.gelu=GELU()\n",
        "    self.fc2=nn.Linear(hidden_size,d_in)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "  def forward(self,x):\n",
        "    x=self.fc1(x)\n",
        "    x=self.gelu(x)\n",
        "    x=self.fc2(x)\n",
        "    x=self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "0LKuylosNz9X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,d_out,eps=1e-6):\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    self.d_out=d_out\n",
        "    self.gamma=nn.Parameter(torch.ones(d_out))\n",
        "    self.beta=nn.Parameter(torch.zeros(d_out))\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Shape ---> (batch, num_tokens,d_out)\n",
        "    mean=x.mean(dim=-1,keepdim=True)\n",
        "    std=x.std(dim=-1,keepdim=True)\n",
        "    return ((x-mean)/(std+self.eps))*self.gamma + self.beta\n"
      ],
      "metadata": {
        "id": "RJeTN9NebDjX"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, d_in, d_out, num_heads, mlp_size, dropout):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_in, d_out, num_heads, dropout)\n",
        "        self.norm1 = torch.nn.LayerNorm(d_in)\n",
        "        self.norm2 = torch.nn.LayerNorm(d_out)\n",
        "        self.mlp   = MLP(d_out, mlp_size, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1) Pre‐attention norm\n",
        "        x_norm = self.norm1(x)                # (batch, n_tokens, d_in)\n",
        "        x_attn = self.mha(x_norm)             # (batch, n_tokens, d_out)\n",
        "\n",
        "        # 2) Residual (project x up only if needed)\n",
        "        res = x             # (batch, n_tokens, d_out)\n",
        "        x = x_attn + res                      # (batch, n_tokens, d_out)\n",
        "\n",
        "        # 3) Pre‐FFN norm\n",
        "        x_norm = self.norm2(x)                # (batch, n_tokens, d_out)\n",
        "        x_ffn  = self.mlp(x_norm)             # (batch, n_tokens, d_out)\n",
        "\n",
        "        # 4) FFN residual\n",
        "        return x + x_ffn\n"
      ],
      "metadata": {
        "id": "7QP4jn03Q-IO"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_in,d_out,num_heads,mlp_size,dropout,num_blocks):\n",
        "    super().__init__()\n",
        "    self.blocks=nn.ModuleList([Block(d_in,d_out,num_heads,mlp_size,dropout) for _ in range(num_blocks)])\n",
        "  def forward(self,x):\n",
        "    for block in self.blocks:\n",
        "      x=block(x)\n",
        "    return x          # Shape ---> (batch, num_tokens+1,d_out)"
      ],
      "metadata": {
        "id": "KJXU-6Rhe4EI"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch=4\n",
        "num_tokens=5\n",
        "emb_dim=768\n",
        "example=torch.randn(batch,num_tokens,emb_dim)\n",
        "print(example.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoEkQgtOfg1P",
        "outputId": "1a00271c-7361-49cb-94a5-f8f9f5d3aa38"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 5, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=Encoder(d_in=emb_dim,d_out=768,num_heads=16,mlp_size=3072,dropout=0.1,num_blocks=12)\n",
        "print(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5VkhiUCgDU2",
        "outputId": "b3b2e09d-ad21-4fa4-d79b-c30e406e2af4"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (blocks): ModuleList(\n",
            "    (0-11): 12 x Block(\n",
            "      (mha): MultiHeadAttention(\n",
            "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): MLP(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (gelu): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=encoder(example)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJlzaoixgWhN",
        "outputId": "e9c0a331-907e-435d-b7f9-c336068fb6cc"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 5, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-cccdbe5cf279>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(0.5*x*(1+torch.tanh(torch.tensor(2/math.pi)**0.5)*(x+0.044715*torch.pow(x,3.0))))\n"
          ]
        }
      ]
    }
  ]
}