{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUUbPJbJwEQW8L5XzuVdos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhavrathore1999/ML-building-blocks/blob/main/Cross_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "U2qlAQN7g3vz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hkGyCuhBgn-_"
      },
      "outputs": [],
      "source": [
        "def entropy(p):\n",
        "  p=np.array(p)\n",
        "  p=p[p>0]\n",
        "  return -np.sum(p*np.log2(p))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_log2(x):\n",
        "    return np.log2(x + 1e-12)  # small constant to avoid log(0)\n",
        "\n",
        "def joint_entropy(joint_pxy):\n",
        "    joint_pxy = np.array(joint_pxy)\n",
        "    mask = joint_pxy > 0\n",
        "    return -np.sum(joint_pxy[mask] * safe_log2(joint_pxy[mask]))"
      ],
      "metadata": {
        "id": "dJzYcKv_jKq_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Entropy:\n",
        "“How many bits (or nats) you need to encode messages drawn from p, using your model q?”"
      ],
      "metadata": {
        "id": "W-73Nzk-ilQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(p,q):\n",
        "  p=np.array(p)\n",
        "  q=np.array(q)\n",
        "  p=p[p>0]\n",
        "  q=q[q>0]\n",
        "  return -np.sum(p*np.log2(q))"
      ],
      "metadata": {
        "id": "iEhEWqCHhO1N"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KL Divergence = Cross-Entropy - Entropy"
      ],
      "metadata": {
        "id": "lUvYpgLJif3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KL Divergence:\n",
        "“How many extra bits you waste using your model\n",
        "q instead of the optimal encoding from the oracle’s\n",
        "p?"
      ],
      "metadata": {
        "id": "3vnVym_OiuMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KL_Divergence(p,q):\n",
        "  p=np.array(p)\n",
        "  q=np.array(q)\n",
        "  p=p[p>0]\n",
        "  q=q[q>0]\n",
        "  return np.sum(p*np.log2(p/q))"
      ],
      "metadata": {
        "id": "yFHnMpn0hqHG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jensen_shannon(p, q):\n",
        "    p = np.array(p)\n",
        "    q = np.array(q)\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * KL_Divergence(p, m) + 0.5 * KL_Divergence(q, m)"
      ],
      "metadata": {
        "id": "rNEXlfdIh9vD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_dist=[0.1,0.3,0.2,0.4]\n",
        "pred_dist=[0.2,0.3,0.4,0.1]\n",
        "print(entropy(true_dist))\n",
        "print(\"Cross Entropy-->\",cross_entropy(true_dist,pred_dist))\n",
        "print(\"KL Divergence-->\",KL_Divergence(true_dist,pred_dist))\n",
        "print(\"Jensen Shannon Divergence\",jensen_shannon(true_dist,pred_dist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDZqjYRghEG1",
        "outputId": "0756e553-0f35-4413-ed05-890208f91bca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8464393446710154\n",
            "Cross Entropy--> 2.3464393446710154\n",
            "KL Divergence--> 0.5\n",
            "Jensen Shannon Divergence 0.10628485095363911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joint_pxy = [\n",
        "    [0.2, 0.3],   # P(X=0, Y=0) and P(X=0, Y=1)\n",
        "    [0.1, 0.4]    # P(X=1, Y=0) and P(X=1, Y=1)\n",
        "]\n",
        "print(\"Joint Entropy-->\",joint_entropy(joint_pxy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP_r5tOXhve0",
        "outputId": "c6ddeab0-820c-4439-bb47-c183c9271223"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Entropy--> 1.8464393446652445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Entropy Loss\n",
        "## Binary Cross Entropy Loss"
      ],
      "metadata": {
        "id": "wNnDMwF4kq2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "PcZ6cNuzj7st"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy_loss(logits,targets):\n",
        "  logits=np.array(logits)\n",
        "  preds=sigmoid(logits)\n",
        "  targets=np.array(targets)\n",
        "  return -np.mean(targets*np.log(preds+1e-12)+(1-targets)*np.log(1-preds+1e-12))"
      ],
      "metadata": {
        "id": "yaYsqfmPk8Kh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For n examples\n",
        "logits = np.array([0.2, 2.0, -1.0])      # model scores\n",
        "targets = np.array([0, 1, 0])            # true labels (binary)\n",
        "\n",
        "loss = binary_cross_entropy_loss(logits, targets)\n",
        "print(f\"Binary Cross-Entropy Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzjG8pFzmUYe",
        "outputId": "37784a48-9097-4efb-b571-97edcd4b0c50"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross-Entropy Loss: 0.4128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi - Class Cross Entropy Loss"
      ],
      "metadata": {
        "id": "1yyJU4Txo0fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    e_logits = np.exp(logits - np.max(logits))  # Shape ---> (N,C)\n",
        "    return e_logits / np.sum(e_logits, axis=-1, keepdims=True)      # Shape ---> (N,C)"
      ],
      "metadata": {
        "id": "rOHkeCuQovYb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(logits,targets):\n",
        "  # Shape --> logits --> (N,C)\n",
        "  # Shape targets -----> (N,)\n",
        "  logits=np.array(logits)\n",
        "  preds=softmax(logits)         # Shape preds --> (N,C)\n",
        "  targets=np.array(targets)     # Shape targets --- > (N,)\n",
        "  correct_log_probs = []\n",
        "  for i in range(len(targets)):\n",
        "      correct_class = targets[i]         # the correct class index for example i\n",
        "      prob = preds[i][correct_class]         # get predicted probability for correct class\n",
        "      loss = -np.log(prob)                   # negative log-likelihood\n",
        "      correct_log_probs.append(loss)\n",
        "\n",
        "  correct_log_probs = np.array(correct_log_probs)\n",
        "  loss = np.sum(correct_log_probs) / len(targets)\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "HwnWHi0QpSzO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = [\n",
        "    [0.7, 0.2, 0.1],   # sample 1\n",
        "    [0.1, 0.8, 0.1],   # sample 2\n",
        "    [0.2, 0.1, 0.7]    # sample 3\n",
        "]\n",
        "true_labels = [0, 1, 2]  # correct class for each sample\n",
        "print(\"Cross Entropy Loss\",cross_entropy_loss(probs,true_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMMjMIIeskia",
        "outputId": "3367e22e-29dd-42bf-eab6-a1e5ae4a1df1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy Loss 0.7418752462591555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQn0Xp7nsvz_"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}